{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-portsmouth",
   "metadata": {},
   "source": [
    "# Data Prep for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_mod = pd.read_csv(\"./Hawaii_nkm_v1.csv.gz\", compression='gzip')\n",
    "SD_mod = pd.read_csv(\"./SanDiego_nkm_v1.csv.gz\", compression='gzip')\n",
    "\n",
    "display(HI_mod.head(5))\n",
    "display(SD_mod.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-victim",
   "metadata": {},
   "source": [
    "### Some Extra Feature Engineering and Data Re-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Hawaii First\n",
    "###############################################################################################################################\n",
    "\n",
    "HI_mod['combo'] = HI_mod[['Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday',\n",
    "                          'Summer_weekend','Winter_weekday', 'Winter_weekend',]].mean(axis=1)\n",
    "HI_mod['price'] = HI_mod['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "HI_mod[['baths','scrap']] = HI_mod['bathrooms_text'].str.split(\" \", 1, expand=True).replace(['Half-bath', 'Private', 'Shared'], '.5')\n",
    "HI_mod['rev_perunit_perDay'] = HI_mod['combo']*HI_mod['price']\n",
    "HI_mod['num_amenities'] = HI_mod['amenities'].str.len()\n",
    "\n",
    "HI_mod = HI_mod.merge(\n",
    "        HI_mod.groupby(['neighbourhood_group','accommodates']).agg({'price':'mean'})\\\n",
    "        .reset_index().rename(columns={'price':'neighbourhood_avg_price'})\n",
    "        ,on=['neighbourhood_group','accommodates'] ,how='inner')\n",
    "\n",
    "###############################################################################################################################\n",
    "# San Diego Second\n",
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "SD_mod['combo'] = SD_mod[['Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday',\n",
    "                          'Summer_weekend','Winter_weekday', 'Winter_weekend',]].mean(axis=1)\n",
    "SD_mod['price'] = SD_mod['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "SD_mod['rev_perunit_perDay'] = SD_mod['combo']*SD_mod['price']\n",
    "\n",
    "SD_mod = SD_mod.merge(\n",
    "        SD_mod.groupby(['neighbourhood']).agg({'price':'mean'})\\\n",
    "        .reset_index().rename(columns={'price':'neighbourhood_avg_price'})\n",
    "        ,on=['neighbourhood'] ,how='inner')\n",
    "\n",
    "SD_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Hawaii First\n",
    "###############################################################################################################################\n",
    "HI_mod_gb = HI_mod.groupby(['listing_id','neighbourhood_group','room_type','Occupancy','NoVacancy', 'combo','price','rev_perunit_perDay',\n",
    "                            'Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday', 'Summer_weekend',\n",
    "                            'Winter_weekday', 'Winter_weekend', 'Fall', 'Spring', 'Summer', 'Winter', 'Weekday', 'Weekend',\n",
    "                            'accommodates', 'baths', 'bedrooms', 'beds', 'num_amenities', 'neighbourhood_avg_price'\n",
    "                           ])\\\n",
    "        .agg({'POI_name':pd.Series.nunique}).reset_index()\\\n",
    "        .sort_values(by=['POI_name'], ascending=False).rename(columns={'POI_name':'Num_POI_within_1mile'})\n",
    "HI_mod_gb\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "# San Diego Second\n",
    "###############################################################################################################################\n",
    "\n",
    "SD_mod_gb = SD_mod.groupby(['listing_id','neighbourhood','room_type','Occupancy','NoVacancy', 'combo','price','rev_perunit_perDay',\n",
    "                            'Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday', 'Summer_weekend',\n",
    "       'Winter_weekday', 'Winter_weekend', 'Fall', 'Spring', 'Summer', 'Winter', 'Weekday', 'Weekend','neighbourhood_avg_price'])\\\n",
    "        .agg({'POI_name':pd.Series.nunique}).reset_index()\\\n",
    "        .sort_values(by=['POI_name'], ascending=False).rename(columns={'POI_name':'Num_POI_within_1mile'})\n",
    "HI_mod_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Hawaii First\n",
    "###############################################################################################################################\n",
    "\n",
    "HI_mod_gb_ = HI_mod_gb\n",
    "for ele in ['neighbourhood_group','room_type', ]:\n",
    "    HI_mod_gb_ = pd.get_dummies(HI_mod_gb_, columns=[ele], prefix=[ele] )\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "# San Diego Second\n",
    "###############################################################################################################################\n",
    "\n",
    "SD_mod_gb_ = SD_mod_gb\n",
    "for ele in ['neighbourhood','room_type', ]:\n",
    "    SD_mod_gb_ = pd.get_dummies(SD_mod_gb_, columns=[ele], prefix=[ele] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-import",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "serious-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Some code brought in from outside, sourced by nkmartin\n",
    "###############################################################################################################################\n",
    "\n",
    "def my_feature_importance(model=None):\n",
    "    \n",
    "    feature_importance = model.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    plt.figure(figsize=(8, 18))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X_train.keys()[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.savefig('VariableImportance.png')\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "def my_quick_profile(df:pd.DataFrame=None, col:str=None):\n",
    "    \n",
    "    print(col)\n",
    "    print('max:',   df[col].max())\n",
    "    print('q85th:', df[col].quantile(.85))\n",
    "    print('q75th:', df[col].quantile(.75))\n",
    "    print('mean:',  df[col].mean())\n",
    "    print('median:',df[col].median())\n",
    "    print('q25th:', df[col].quantile(.25))\n",
    "    print('q15th:', df[col].quantile(.15))\n",
    "    print('min:',   df[col].min())\n",
    "    \n",
    "    return df[col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-immune",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-fancy",
   "metadata": {},
   "source": [
    "### Basic Model - Neighbourhood only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(SD_mod_gb_.columns) - set(['listing_id','Occupancy', 'NoVacancy', 'combo', 'rev_perunit_perDay',\n",
    "                                 'Fall', 'Spring', 'Summer', 'Winter','Weekday','Weekend','Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday', 'Summer_weekend',\n",
    "                            'Winter_weekday', 'Winter_weekend',]))\n",
    "\n",
    "X = SD_mod_gb_[features]\n",
    "\n",
    "y = SD_mod_gb_['NoVacancy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "\n",
    "\n",
    "SD_reg = RandomForestRegressor(random_state=0, n_estimators=100, max_depth= 12, min_samples_split=5).fit(X_train, y_train)\n",
    "SD_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_importance(SD_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-defense",
   "metadata": {},
   "source": [
    "### Better Model - Neighbourhood and Unit Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(HI_mod_gb_.columns) - set(['listing_id','Occupancy', 'NoVacancy', 'combo', 'rev_perunit_perDay',\n",
    "                                 'Fall', 'Spring', 'Summer', 'Winter','Weekday','Weekend',\n",
    "                                'Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday', 'Summer_weekend','Winter_weekday', 'Winter_weekend'\n",
    "                                               ]))\n",
    "\n",
    "X = HI_mod_gb_[features]\n",
    "\n",
    "y = HI_mod_gb_['NoVacancy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "\n",
    "\n",
    "HI_reg = RandomForestRegressor(random_state=0, n_estimators=100, max_depth= 12, min_samples_split=5).fit(X_train, y_train)\n",
    "HI_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_importance(HI_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-diameter",
   "metadata": {},
   "source": [
    "### Better Model - Neighbourhood, Unit Specific, and Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(HI_mod_gb_.columns) - set(['listing_id','Occupancy', 'NoVacancy', 'combo', 'rev_perunit_perDay',\n",
    "                                 'Fall', 'Spring', 'Summer', 'Winter','Weekday','Weekend',\n",
    "                                 #'Fall_weekday', 'Fall_weekend', 'Spring_weekday', 'Spring_weekend', 'Summer_weekday', 'Summer_weekend','Winter_weekday', 'Winter_weekend'\n",
    "                                               ]))\n",
    "\n",
    "X = HI_mod_gb_[features]\n",
    "\n",
    "y = HI_mod_gb_['Occupancy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "\n",
    "\n",
    "HI_reg_ = RandomForestRegressor(random_state=0, n_estimators=100, max_depth= 12, min_samples_split=5).fit(X_train, y_train)\n",
    "HI_reg_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_importance(HI_reg_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-pitch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-calvin",
   "metadata": {},
   "source": [
    "# Grid Search to Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Some code brought in from outside, sourced by nkmartin\n",
    "###############################################################################################################################\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        print(\"self.params \", self.params )\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores)\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-illinois",
   "metadata": {},
   "source": [
    "### Choose Models and the Parameter Values to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Please note: The more combinations you add, the longer this cell will run.  Running multiple hours is not uncommon  \n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "# Parallelizable version available for cluster use (not needed for this project size)\n",
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "models1 = {\n",
    "#     'LogisticRegression':LogisticRegression(),\n",
    "#     'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "#     'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "#     'GradientBoostingRegressor': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "#     'LogisticRegression': {'solver':['lbfgs','liblinear'],'random_state':[0]},\n",
    "#     'ExtraTreesRegressor': { 'n_estimators': [(i+1)*150  for i in range(10)],'max_depth': [(i+1)*5 for i in range(6)] },\n",
    "    'RandomForestRegressor': { 'n_estimators': [(i+1)*50 for i in range(3)],'max_depth': [(i+3)*2 for i in range(6)], 'random_state':[0],'min_samples_split':[5, 15, 30] },\n",
    "#     'AdaBoostRegressor':  { 'n_estimators': [(i+1)*50  for i in range(3)], 'learning_rate': [ 0.1, 0.05, .01] },\n",
    "#     'GradientBoostingRegressor': { 'n_estimators': [(i+1)*50  for i in range(3)],'min_samples_split':[35, 10, 25], 'max_depth': [(i)*3+7 for i in range(3)], 'learning_rate': [ 0.015, .05],  'random_state':[0] },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-priest",
   "metadata": {},
   "source": [
    "### Execute Grid Search - this cell may run very slow based on combinations above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = EstimatorSelectionHelper(models1, params1)\n",
    "helper.fit(X_train, y_train, scoring='r2', refit='r2', n_jobs=-1)\n",
    "helper.score_summary(sort_by='mean_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-moral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
